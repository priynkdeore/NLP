{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a463353-139b-4e81-9d0a-121dc1609be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1f1585-133a-4c1c-a87d-18631c10215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb06802-4d12-4efd-96be-8d80f9bdab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"John Doe works at OpenAI in San Francisco. He loves programming and exploring new AI technologies.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cead544-dcce-47a8-9ee4-710c2bf66783",
   "metadata": {},
   "source": [
    "# Tokenization:\n",
    "\n",
    "# Splitting text into individual words or phrases.\n",
    "# Example: \"Natural Language Processing\" -> [\"Natural\", \"Language\", \"Processing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6793be2a-ef7e-478c-be5f-7fd9bac0864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'Doe', 'works', 'at', 'OpenAI', 'in', 'San', 'Francisco', '.', 'He', 'loves', 'programming', 'and', 'exploring', 'new', 'AI', 'technologies', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c583fdf-2097-4482-8550-25bfd4b2cf05",
   "metadata": {},
   "source": [
    "# Stop Words Removal:\n",
    "\n",
    "# Removing common words that do not contribute much to the meaning (e.g., \"and\", \"the\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d64546f-cbc8-4eba-b77e-77673b50a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['John', 'Doe', 'works', 'OpenAI', 'San', 'Francisco', '.', 'loves', 'programming', 'exploring', 'new', 'AI', 'technologies', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913c0f7-7df4-4e5c-8227-948ad23274f0",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization:\n",
    "\n",
    "# Reducing words to their root form.\n",
    "# Stemming: \"running\" -> \"run\"\n",
    "# Lemmatization: \"running\" -> \"run\" (more accurate, considers context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10902f1-51f9-4188-840e-5b4986fced14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['john', 'doe', 'work', 'openai', 'san', 'francisco', '.', 'love', 'program', 'explor', 'new', 'ai', 'technolog', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cdbd2-9e09-46bb-9c19-233399cb7c0d",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) Tagging:\n",
    "\n",
    "# Identifying the grammatical category of each word.\n",
    "# Example: \"Natural\" (Adjective), \"Language\" (Noun), \"Processing\" (Verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d389f982-4c59-4965-afaa-4d6998f1dbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('John', 'NNP'), ('Doe', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('OpenAI', 'NNP'), ('in', 'IN'), ('San', 'NNP'), ('Francisco', 'NNP'), ('.', '.'), ('He', 'PRP'), ('loves', 'VBZ'), ('programming', 'VBG'), ('and', 'CC'), ('exploring', 'VBG'), ('new', 'JJ'), ('AI', 'NNP'), ('technologies', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5c7d2-2a90-4def-8e75-8d079626ff4c",
   "metadata": {},
   "source": [
    "# POS Tags Explained\n",
    "NNP: Proper noun, singular\n",
    "VBZ: Verb, 3rd person singular present\n",
    "IN: Preposition or subordinating conjunction\n",
    ".: Punctuation mark, period\n",
    "PRP: Personal pronoun\n",
    "VBG: Verb, gerund or present participle\n",
    "CC: Coordinating conjunction\n",
    "JJ: Adjective\n",
    "NNS: Noun, plural\n",
    "\n",
    "# The POS tags help in understanding the syntactic structure of the sentence. They can be used for various NLP tasks such as:\n",
    "\n",
    "# Parsing: Building a syntactic parse tree of the sentence.\n",
    "# Information Extraction: Identifying entities, relations, and events in text.\n",
    "# Text-to-Speech: Determining the correct pronunciation based on word context.\n",
    "# Machine Translation: Ensuring correct grammatical structure in translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e282e-d285-478c-81a2-761d79a38314",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER):\n",
    "\n",
    "# Identifying and classifying named entities in text (e.g., names, dates, locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fb5ec1-9dfb-4e29-978c-00a45402b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  (PERSON John/NNP)\n",
      "  (ORGANIZATION Doe/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION OpenAI/NNP)\n",
      "  in/IN\n",
      "  (GPE San/NNP Francisco/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  loves/VBZ\n",
      "  programming/VBG\n",
      "  and/CC\n",
      "  exploring/VBG\n",
      "  new/JJ\n",
      "  AI/NNP\n",
      "  technologies/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Named Entity Recognition (NER)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24881f3c-570e-4a2a-ad8d-632e9af55940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "named_entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb80047a-2dfa-478e-afb0-559fc0597d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix:\n",
      " [[0 0 0 1 0 0 0 0 0 1 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 1 0 1 0 0 0 1]\n",
      " [1 1 0 0 1 0 1 1 0 0 0 0 1 0 0]]\n",
      "Feature Names:\n",
      " ['and' 'are' 'ate' 'cat' 'cats' 'dog' 'dogs' 'great' 'homework' 'mat' 'my'\n",
      " 'on' 'pets' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog ate my homework\",\n",
    "    \"Cats and dogs are great pets\"\n",
    "]\n",
    "\n",
    "# Create the Bag of Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense format and print\n",
    "print(\"Bag of Words Matrix:\\n\", X.toarray())\n",
    "\n",
    "# Print the feature names (words)\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4ac9d3-651e-4241-affc-c462236a83b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.39798027 0.         0.\n",
      "  0.         0.         0.         0.39798027 0.         0.39798027\n",
      "  0.         0.39798027 0.60534851]\n",
      " [0.         0.         0.46735098 0.         0.         0.46735098\n",
      "  0.         0.         0.46735098 0.         0.46735098 0.\n",
      "  0.         0.         0.35543247]\n",
      " [0.40824829 0.40824829 0.         0.         0.40824829 0.\n",
      "  0.40824829 0.40824829 0.         0.         0.         0.\n",
      "  0.40824829 0.         0.        ]]\n",
      "Feature Names:\n",
      " ['and' 'are' 'ate' 'cat' 'cats' 'dog' 'dogs' 'great' 'homework' 'mat' 'my'\n",
      " 'on' 'pets' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog ate my homework\",\n",
    "    \"Cats and dogs are great pets\"\n",
    "]\n",
    "\n",
    "# Create the TF-IDF model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense format and print\n",
    "print(\"TF-IDF Matrix:\\n\", X.toarray())\n",
    "\n",
    "# Print the feature names (words)\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb334f04-4fe9-418d-808b-54848d06fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensimNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/ab/b0/d58dc405fd60ab546ca714321235dc2d455b2dc06bfb4fc1092940c749fc/gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.25.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/24.0 MB 6.9 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.5/24.0 MB 6.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.7/24.0 MB 6.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.9/24.0 MB 5.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.1/24.0 MB 6.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.2/24.0 MB 5.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.4/24.0 MB 5.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.7/24.0 MB 5.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.0 MB 5.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.9/24.0 MB 5.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.3/24.0 MB 5.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.4/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.9/24.0 MB 5.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.0/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.0/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.2/24.0 MB 4.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.5/24.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.8/24.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.9/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.0/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.2/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.2/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.4/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.4/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.7/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 4.8/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 4.9/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.0/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.2/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.3/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 5.5/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 5.8/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 5.9/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.4/24.0 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.5/24.0 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 4.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.6/24.0 MB 4.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.6/24.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.7/24.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.9/24.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.2/24.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.4/24.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.4/24.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.6/24.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.8/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.0/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.2/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.3/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.4/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.6/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.7/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.2/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.3/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.6/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 9.8/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 9.9/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.0/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.2/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.4/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.6/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 10.8/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.0/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.2/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.4/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.5/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.7/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.9/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.1/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.3/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.5/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.7/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.0/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.4/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.4/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.6/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.8/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.0/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.5/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.6/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.8/24.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.1/24.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.6/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 15.8/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.0/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.3/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.6/24.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 16.9/24.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.2/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.5/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.7/24.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.9/24.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.0/24.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.2/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.3/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.5/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.6/24.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.7/24.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.9/24.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.0 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.3/24.0 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.6/24.0 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.8/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.0/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.2/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.9/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.1/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.3/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.4/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.5/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.8/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.2/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.4/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.6/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.0 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.0/24.0 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.5/24.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.7/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.8/24.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84e7ab5f-94a9-4928-9555-9bfbd84cc2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'cat':\n",
      " [ 7.6966463e-03  9.1206422e-03  1.1355019e-03 -8.3250795e-03\n",
      "  8.4250160e-03 -3.6962307e-03  5.7421732e-03  4.3915794e-03\n",
      "  9.6899448e-03 -9.2934975e-03  9.2084054e-03 -9.2815282e-03\n",
      " -6.9077122e-03 -9.1021946e-03 -5.5471100e-03  7.3688962e-03\n",
      "  9.1644777e-03 -3.3253515e-03  3.7230505e-03 -3.6252034e-03\n",
      "  7.8814710e-03  5.8668759e-03  2.0861626e-07 -3.6286747e-03\n",
      " -7.2243060e-03  4.7686161e-03  1.4529788e-03 -2.6131857e-03\n",
      "  7.8378068e-03 -4.0496145e-03 -9.1489861e-03 -2.2554707e-03\n",
      "  1.2514711e-04 -6.6392552e-03 -5.4866159e-03 -8.4997769e-03\n",
      "  9.2298733e-03  7.4240281e-03 -2.9524326e-04  7.3676636e-03\n",
      "  7.9507884e-03 -7.8357337e-04  6.6120909e-03  3.7675237e-03\n",
      "  5.0768424e-03  7.2529912e-03 -4.7393893e-03 -2.1855331e-03\n",
      "  8.7312341e-04  4.2362059e-03  3.3043313e-03  5.0958274e-03\n",
      "  4.5864857e-03 -8.4385090e-03 -3.1838394e-03 -7.2367596e-03\n",
      "  9.6814223e-03  5.0065992e-03  1.7084122e-04  4.1129780e-03\n",
      " -7.6561309e-03 -6.2946510e-03  3.0763936e-03  6.5346383e-03\n",
      "  3.9498745e-03  6.0180221e-03 -1.9861318e-03 -3.3451295e-03\n",
      "  2.0717025e-04 -3.1943608e-03 -5.5169044e-03 -7.7885604e-03\n",
      "  6.5355431e-03 -1.0903371e-03 -1.8908798e-03 -7.8047751e-03\n",
      "  9.3375733e-03  8.6814165e-04  1.7696369e-03  2.4916660e-03\n",
      " -7.3859929e-03  1.6388226e-03  2.9765631e-03 -8.5670296e-03\n",
      "  4.9558021e-03  2.4334085e-03  7.4979127e-03  5.0442982e-03\n",
      " -3.0317164e-03 -7.1629370e-03  7.0962133e-03  1.9015349e-03\n",
      "  5.1992359e-03  6.3811089e-03  1.9122792e-03 -6.1276113e-03\n",
      " -6.2966346e-06  8.2682976e-03 -6.0985480e-03  9.4382809e-03]\n",
      "Words most similar to 'cat':\n",
      " [('are', 0.17826786637306213), ('pets', 0.16072483360767365), ('homework', 0.10560770332813263), ('on', 0.09215974807739258), ('sat', 0.027008360251784325), ('cats', 0.00773055013269186), ('the', -0.037719566375017166), ('ate', -0.045522745698690414), ('dog', -0.04649786278605461), ('mat', -0.05902623012661934)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"ate\", \"my\", \"homework\"],\n",
    "    [\"cats\", \"and\", \"dogs\", \"are\", \"great\", \"pets\"]\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "vector = model.wv['cat']\n",
    "print(\"Word Vector for 'cat':\\n\", vector)\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar('cat')\n",
    "print(\"Words most similar to 'cat':\\n\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd25e15-c818-422e-8086-13f427fb5e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/6a/dc/23c26b7b0bce5aaccf2b767db3e9c4f5ae4331bd47688c1f2ef091b23696/transformers-4.42.4-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/43.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 43.6/43.6 kB 355.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.2 from https://files.pythonhosted.org/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/28/78/82c03572b085b658d24a13ae0ae4779c0712a167af6c313d39ee5b5e7f73/safetensors-0.4.3-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/f4/85/d999b9a05fd101d48f1a365d68be0b109277bb25c89fb37a389d669f9185/tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/5e/44/73bea497ac69bafde2ee4269292fa3b41f1198f4bb7bbaaabde30ad29d4a/fsspec-2024.6.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.3 MB 6.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/9.3 MB 8.8 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.9/9.3 MB 8.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/9.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.3 MB 7.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.3 MB 6.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.6/9.3 MB 6.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.3 MB 6.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/9.3 MB 5.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/9.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/9.3 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/9.3 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.5/9.3 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/9.3 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.8/9.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.3 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.1/9.3 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.3/9.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.3 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.3 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.6/9.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.8/9.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.1/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.4/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.6/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.8/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.8/9.3 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.0/9.3 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.1/9.3 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.3/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.5/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.6/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.7/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.3 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.4/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.7/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.4/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.7/9.3 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.5/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.3 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
      "   --------------------------------------  399.4/402.6 kB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  399.4/402.6 kB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 402.6/402.6 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   ---------------------------------------  286.7/287.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  286.7/287.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  286.7/287.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 287.4/287.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.7/2.2 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 12.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   --------------------------------------  174.1/177.6 kB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 177.6/177.6 kB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "509a22af-47ec-4c8d-9a54-deee924ed395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " [{'generated_text': 'The future of AI is in the hands of people for free. And with a computer, computers are actually more powerful. You need a smart machine to do everything, or you need a computer to solve problems with a human. And now, with AI'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted masked word:\n",
      " hit\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of AI is\"\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "\n",
    "# BERT for masked word prediction\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text and create mask\n",
    "# text = \"The quick brown fox [MASK] over the lazy dog.\"\n",
    "text = \"Dang! I’m out fishing and a huge trout just [MASK] my line!\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Predict masked word\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    mask_token_logits = output.logits[0, mask_token_index, :]\n",
    "    mask_token_id = torch.argmax(mask_token_logits, dim=1)\n",
    "    predicted_token = tokenizer.decode(mask_token_id)\n",
    "\n",
    "print(\"Predicted masked word:\\n\", predicted_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ec497-62c0-437a-a9c3-fadb49dc00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature names (words)\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31734f0-4a86-4c31-98e9-74f7f2826aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 128)         98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2757761 (10.52 MB)\n",
      "Trainable params: 2757761 (10.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "def get_model():\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "    # Embed each integer in a 128-dimensional vector\n",
    "    x = layers.Embedding(max_features, 128)(inputs)\n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    # Add a classifier\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1a2c8c-3406-4cdc-b14f-35bd78f3b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a949c3-d8f4-44f8-915b-302f2428035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "782/782 [==============================] - 1823s 2s/step - loss: 0.4257 - accuracy: 0.8012 - val_loss: 0.3560 - val_accuracy: 0.8586\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 12452s 16s/step - loss: 0.2081 - accuracy: 0.9216 - val_loss: 0.3334 - val_accuracy: 0.8712\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 41999s 54s/step - loss: 0.1189 - accuracy: 0.9573 - val_loss: 0.4085 - val_accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d42f6b71f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8c03e-3ee2-4e4c-9e66-738258c0f106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
